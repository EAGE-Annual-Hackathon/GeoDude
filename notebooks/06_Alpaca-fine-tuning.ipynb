{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "933b62b5",
   "metadata": {},
   "source": [
    "## Alpaca LoRA fine-tuning for Petrobowl dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd026bf3",
   "metadata": {},
   "source": [
    "This notebook needs more packages to be installed, please follow it and proceed to install the needed packagaes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af00bc87",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (23.0.1)\n",
      "Collecting pip\n",
      "  Downloading pip-23.1.2-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.0.1\n",
      "    Uninstalling pip-23.0.1:\n",
      "      Successfully uninstalled pip-23.0.1\n",
      "Successfully installed pip-23.1.2\n",
      "Collecting accelerate==0.18.0\n",
      "  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from accelerate==0.18.0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from accelerate==0.18.0) (23.1)\n",
      "Requirement already satisfied: psutil in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from accelerate==0.18.0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from accelerate==0.18.0) (6.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from accelerate==0.18.0) (2.0.1)\n",
      "Requirement already satisfied: filelock in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from torch>=1.4.0->accelerate==0.18.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from torch>=1.4.0->accelerate==0.18.0) (4.5.0)\n",
      "Requirement already satisfied: sympy in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from torch>=1.4.0->accelerate==0.18.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from torch>=1.4.0->accelerate==0.18.0) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from torch>=1.4.0->accelerate==0.18.0) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from jinja2->torch>=1.4.0->accelerate==0.18.0) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from sympy->torch>=1.4.0->accelerate==0.18.0) (1.2.1)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.18.0\n",
      "Collecting appdirs==1.4.4\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Installing collected packages: appdirs\n",
      "Successfully installed appdirs-1.4.4\n",
      "Collecting bitsandbytes==0.37.2\n",
      "  Downloading bitsandbytes-0.37.2-py3-none-any.whl (84.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.37.2\n",
      "Collecting datasets==2.10.1\n",
      "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from datasets==2.10.1) (1.24.3)\n",
      "Collecting pyarrow>=6.0.0 (from datasets==2.10.1)\n",
      "  Downloading pyarrow-12.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.9/38.9 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.7,>=0.3.0 (from datasets==2.10.1)\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from datasets==2.10.1) (2.0.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from datasets==2.10.1) (2.29.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from datasets==2.10.1) (4.65.0)\n",
      "Collecting xxhash (from datasets==2.10.1)\n",
      "  Downloading xxhash-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.2/213.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets==2.10.1)\n",
      "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1 (from datasets==2.10.1)\n",
      "  Using cached fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
      "Collecting aiohttp (from datasets==2.10.1)\n",
      "  Downloading aiohttp-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.2.0 (from datasets==2.10.1)\n",
      "  Using cached huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "Requirement already satisfied: packaging in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from datasets==2.10.1) (23.1)\n",
      "Collecting responses<0.19 (from datasets==2.10.1)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from datasets==2.10.1) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from aiohttp->datasets==2.10.1) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from aiohttp->datasets==2.10.1) (2.0.4)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.10.1)\n",
      "  Downloading multidict-6.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.4/117.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets==2.10.1)\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.10.1)\n",
      "  Downloading yarl-1.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.8/282.8 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets==2.10.1)\n",
      "  Downloading frozenlist-1.3.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.3/154.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets==2.10.1)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: filelock in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.1) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.1) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.10.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.10.1) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.10.1) (2023.5.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from pandas->datasets==2.10.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from pandas->datasets==2.10.1) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from pandas->datasets==2.10.1) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.10.1) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.10.1 dill-0.3.6 frozenlist-1.3.3 fsspec-2023.5.0 huggingface-hub-0.15.1 multidict-6.0.4 multiprocess-0.70.14 pyarrow-12.0.0 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n",
      "Collecting fire==0.5.0\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m747.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from fire==0.5.0) (1.16.0)\n",
      "Collecting termcolor (from fire==0.5.0)\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116931 sha256=de8b8a30a2e5c8580e67373c465f5df4bfa8ede1bd288c4bebcecf410a923977\n",
      "  Stored in directory: /home/alfarhmy/.cache/pip/wheels/a7/ee/a5/19e91481be8bea594935d137578bfe77d6bf905e4595336f6b\n",
      "Successfully built fire\n",
      "Installing collected packages: termcolor, fire\n",
      "Successfully installed fire-0.5.0 termcolor-2.3.0\n",
      "Collecting git+https://github.com/huggingface/peft.git\n",
      "  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-qwwu908w\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-qwwu908w\n",
      "  Resolved https://github.com/huggingface/peft.git to commit fcff23f005fc7bfb816ad1f55360442c170cd5f5\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from peft==0.4.0.dev0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from peft==0.4.0.dev0) (23.1)\n",
      "Requirement already satisfied: psutil in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from peft==0.4.0.dev0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from peft==0.4.0.dev0) (6.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from peft==0.4.0.dev0) (2.0.1)\n",
      "Collecting transformers (from peft==0.4.0.dev0)\n",
      "  Using cached transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "Requirement already satisfied: accelerate in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from peft==0.4.0.dev0) (0.18.0)\n",
      "Requirement already satisfied: filelock in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (4.5.0)\n",
      "Requirement already satisfied: sympy in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (3.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from transformers->peft==0.4.0.dev0) (0.15.1)\n",
      "Collecting regex!=2019.12.17 (from transformers->peft==0.4.0.dev0)\n",
      "  Downloading regex-2023.6.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.9/781.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from transformers->peft==0.4.0.dev0) (2.29.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->peft==0.4.0.dev0)\n",
      "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from transformers->peft==0.4.0.dev0) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft==0.4.0.dev0) (2023.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0.dev0) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from requests->transformers->peft==0.4.0.dev0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from requests->transformers->peft==0.4.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from requests->transformers->peft==0.4.0.dev0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from requests->transformers->peft==0.4.0.dev0) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft==0.4.0.dev0) (1.2.1)\n",
      "Building wheels for collected packages: peft\n",
      "  Building wheel for peft (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peft: filename=peft-0.4.0.dev0-py3-none-any.whl size=57784 sha256=19b64d7323d7e5b948ef614009c858d7ac95f3293ad0a5efcea9413bb96fff38\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ds9h5ibe/wheels/42/ec/c4/eb24dac74be83ba2ed4817037a784d1c775e317cb8de69963f\n",
      "Successfully built peft\n",
      "Installing collected packages: tokenizers, regex, transformers, peft\n",
      "Successfully installed peft-0.4.0.dev0 regex-2023.6.3 tokenizers-0.13.3 transformers-4.29.2\n",
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-fczu_aax\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-fczu_aax\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 539e2281cd97c35ef4122757f26c88f44115fa94\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from transformers==4.30.0.dev0) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from transformers==4.30.0.dev0) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from transformers==4.30.0.dev0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from transformers==4.30.0.dev0) (23.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml>=5.1 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from transformers==4.30.0.dev0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from transformers==4.30.0.dev0) (2023.6.3)\n",
      "Requirement already satisfied: requests in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from transformers==4.30.0.dev0) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from transformers==4.30.0.dev0) (0.13.3)\n",
      "Collecting safetensors>=0.3.1 (from transformers==4.30.0.dev0)\n",
      "  Downloading safetensors-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from transformers==4.30.0.dev0) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0.dev0) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0.dev0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from requests->transformers==4.30.0.dev0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from requests->transformers==4.30.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from requests->transformers==4.30.0.dev0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from requests->transformers==4.30.0.dev0) (2023.5.7)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.30.0.dev0-py3-none-any.whl size=7160079 sha256=e081bad396f2eaddbcf942f4928e226f93cb07dd25f6776c9731a32ae79f068c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-jydv078i/wheels/32/4b/78/f195c684dd3a9ed21f3b39fe8f85b48df7918581b6437be143\n",
      "Successfully built transformers\n",
      "Installing collected packages: safetensors, transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.29.2\n",
      "    Uninstalling transformers-4.29.2:\n",
      "      Successfully uninstalled transformers-4.29.2\n",
      "Successfully installed safetensors-0.3.1 transformers-4.30.0.dev0\n",
      "Collecting sentencepiece==0.1.97\n",
      "  Downloading sentencepiece-0.1.97.tar.gz (524 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.7/524.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: sentencepiece\n",
      "  Building wheel for sentencepiece (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentencepiece: filename=sentencepiece-0.1.97-cp311-cp311-linux_x86_64.whl size=1258259 sha256=75bf11b0a68b98b41e5a5429a6b37f922d205775627425485004e1b209cbfdfe\n",
      "  Stored in directory: /home/alfarhmy/.cache/pip/wheels/04/dd/ab/6e3d4b6b17fe7ca0f54548ae20941c40bb3f15839d66f5598c\n",
      "Successfully built sentencepiece\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.97\n",
      "Collecting tensorboardX==2.6\n",
      "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m687.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from tensorboardX==2.6) (1.24.3)\n",
      "Requirement already satisfied: packaging in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from tensorboardX==2.6) (23.1)\n",
      "Collecting protobuf<4,>=3.8.0 (from tensorboardX==2.6)\n",
      "  Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf, tensorboardX\n",
      "Successfully installed protobuf-3.20.3 tensorboardX-2.6\n",
      "Collecting gradio==3.23.0\n",
      "  Downloading gradio-3.23.0-py3-none-any.whl (15.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiofiles (from gradio==3.23.0)\n",
      "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: aiohttp in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from gradio==3.23.0) (3.8.4)\n",
      "Collecting altair>=4.2.0 (from gradio==3.23.0)\n",
      "  Downloading altair-5.0.1-py3-none-any.whl (471 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.5/471.5 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fastapi (from gradio==3.23.0)\n",
      "  Downloading fastapi-0.96.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ffmpy (from gradio==3.23.0)\n",
      "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from gradio==3.23.0) (2023.5.0)\n",
      "Collecting httpx (from gradio==3.23.0)\n",
      "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from gradio==3.23.0) (0.15.1)\n",
      "Requirement already satisfied: jinja2 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from gradio==3.23.0) (3.1.2)\n",
      "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio==3.23.0)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markupsafe in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from gradio==3.23.0) (2.1.1)\n",
      "Requirement already satisfied: matplotlib in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from gradio==3.23.0) (3.7.1)\n",
      "Collecting mdit-py-plugins<=0.3.3 (from gradio==3.23.0)\n",
      "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from gradio==3.23.0) (1.24.3)\n",
      "Collecting orjson (from gradio==3.23.0)\n",
      "  Downloading orjson-3.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.0/137.0 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from gradio==3.23.0) (2.0.2)\n",
      "Requirement already satisfied: pillow in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from gradio==3.23.0) (9.4.0)\n",
      "Collecting pydantic (from gradio==3.23.0)\n",
      "  Downloading pydantic-1.10.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hCollecting pydub (from gradio==3.23.0)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting python-multipart (from gradio==3.23.0)\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from gradio==3.23.0) (6.0)\n",
      "Requirement already satisfied: requests in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from gradio==3.23.0) (2.29.0)\n",
      "Collecting semantic-version (from gradio==3.23.0)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from gradio==3.23.0) (4.5.0)\n",
      "Collecting uvicorn (from gradio==3.23.0)\n",
      "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting websockets>=10.0 (from gradio==3.23.0)\n",
      "  Downloading websockets-11.0.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.6/130.6 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jsonschema>=3.0 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from altair>=4.2.0->gradio==3.23.0) (4.17.3)\n",
      "Collecting toolz (from altair>=4.2.0->gradio==3.23.0)\n",
      "  Using cached toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: filelock in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from huggingface-hub->gradio==3.23.0) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from huggingface-hub->gradio==3.23.0) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from huggingface-hub->gradio==3.23.0) (23.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py[linkify]>=2.0.0->gradio==3.23.0)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify]>=2.0.0->gradio==3.23.0)\n",
      "  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
      "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting mdit-py-plugins<=0.3.3 (from gradio==3.23.0)\n",
      "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
      "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
      "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
      "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
      "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
      "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
      "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
      "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio==3.23.0)\n",
      "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from pandas->gradio==3.23.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from pandas->gradio==3.23.0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from pandas->gradio==3.23.0) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from aiohttp->gradio==3.23.0) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from aiohttp->gradio==3.23.0) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from aiohttp->gradio==3.23.0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from aiohttp->gradio==3.23.0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from aiohttp->gradio==3.23.0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from aiohttp->gradio==3.23.0) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from aiohttp->gradio==3.23.0) (1.3.1)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio==3.23.0)\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from httpx->gradio==3.23.0) (2023.5.7)\n",
      "Collecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio==3.23.0)\n",
      "  Downloading httpcore-0.17.2-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from httpx->gradio==3.23.0) (3.4)\n",
      "Requirement already satisfied: sniffio in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from httpx->gradio==3.23.0) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from matplotlib->gradio==3.23.0) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from matplotlib->gradio==3.23.0) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from matplotlib->gradio==3.23.0) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from matplotlib->gradio==3.23.0) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from matplotlib->gradio==3.23.0) (3.0.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from requests->gradio==3.23.0) (1.26.15)\n",
      "Collecting click>=7.0 (from uvicorn->gradio==3.23.0)\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting h11>=0.8 (from uvicorn->gradio==3.23.0)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio==3.23.0) (3.7.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio==3.23.0) (0.19.3)\n",
      "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio==3.23.0)\n",
      "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->gradio==3.23.0) (1.16.0)\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4693 sha256=f2061d9942056cb25a3186f3e9e9d3c54eead70bbf81d022ee1976ba2f146682\n",
      "  Stored in directory: /home/alfarhmy/.cache/pip/wheels/47/a0/29/b51cefd1cb11f7bd95ec49652e5bda63f17e4718c67280de57\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: pydub, ffmpy, websockets, uc-micro-py, toolz, semantic-version, python-multipart, pydantic, orjson, mdurl, h11, click, aiofiles, uvicorn, starlette, markdown-it-py, linkify-it-py, httpcore, mdit-py-plugins, httpx, fastapi, altair, gradio\n",
      "Successfully installed aiofiles-23.1.0 altair-5.0.1 click-8.1.3 fastapi-0.96.0 ffmpy-0.3.0 gradio-3.23.0 h11-0.14.0 httpcore-0.17.2 httpx-0.24.1 linkify-it-py-2.0.2 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 mdurl-0.1.2 orjson-3.9.0 pydantic-1.10.8 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.27.0 toolz-0.12.0 uc-micro-py-1.0.2 uvicorn-0.22.0 websockets-11.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip\n",
    "!pip install accelerate==0.18.0\n",
    "!pip install appdirs==1.4.4\n",
    "!pip install bitsandbytes\n",
    "!pip install datasets==2.10.1\n",
    "!pip install fire==0.5.0\n",
    "!pip install git+https://github.com/huggingface/peft.git\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "# !pip install torch==2.0.0\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install sentencepiece==0.1.97\n",
    "!pip install tensorboardX==2.6\n",
    "!pip install gradio==3.23.0\n",
    "!pip install seaborn\n",
    "!pip install gdown\n",
    "!pip install scipy\n",
    "!pip install transformers[torch]\n",
    "# !pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab5ca4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/alfarhmy/miniconda3/envs/hackathon2023/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/alfarhmy/miniconda3/envs/hackathon2023/lib/libcudart.so.11.0'), PosixPath('/home/alfarhmy/miniconda3/envs/hackathon2023/lib/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import transformers\n",
    "import textwrap\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    " \n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    ")\n",
    " \n",
    "import fire\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    " \n",
    "%matplotlib inline\n",
    "sns.set(rc={'figure.figsize':(10, 7)})\n",
    "sns.set(rc={'figure.dpi':100})\n",
    "sns.set(style='white', palette='muted', font_scale=1.2)\n",
    " \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcd90c9",
   "metadata": {},
   "source": [
    "## Alpaca LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fead0f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5f0d7089a84f23858dd23a300377a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL = \"decapoda-research/llama-7b-hf\"\n",
    " \n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"sequential\",\n",
    ")\n",
    " \n",
    "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    " \n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eec997e",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "856cb103",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/alfarhmy/.cache/huggingface/datasets/json/default-3d9b400c816737d0/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7583d54e16e6464db92127492dbcdc05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output'],\n",
       "    num_rows: 1274\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"json\", data_files=\"petrobowl_dataset.json\")\n",
    "data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c61ce6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Provide an answer for the petrobowl question',\n",
       " 'input': 'Who extracted oil from asphalt',\n",
       " 'output': 'Abraham Gesner 1846'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4acc57e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  # noqa: E501\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\"\n",
    " \n",
    " \n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < CUTOFF_LEN\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    " \n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    " \n",
    "    return result\n",
    " \n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e23d3495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/alfarhmy/.cache/huggingface/datasets/json/default-3d9b400c816737d0/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-6a612521cd0ae8cc.arrow and /home/alfarhmy/.cache/huggingface/datasets/json/default-3d9b400c816737d0/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-71c8f377ca800729.arrow\n",
      "Loading cached processed dataset at /home/alfarhmy/.cache/huggingface/datasets/json/default-3d9b400c816737d0/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-ed5bf334dd309148.arrow\n",
      "Loading cached processed dataset at /home/alfarhmy/.cache/huggingface/datasets/json/default-3d9b400c816737d0/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-8b0437adb22eef55.arrow\n"
     ]
    }
   ],
   "source": [
    "CUTOFF_LEN = 256\n",
    "\n",
    "train_val = data[\"train\"].train_test_split(\n",
    "    test_size=200, shuffle=True, seed=42\n",
    ")\n",
    "train_data = (\n",
    "    train_val[\"train\"].map(generate_and_tokenize_prompt)\n",
    ")\n",
    "val_data = (\n",
    "    train_val[\"test\"].map(generate_and_tokenize_prompt)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d67a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT= 0.05\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    " \n",
    "BATCH_SIZE = 128\n",
    "MICRO_BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LEARNING_RATE = 3e-4\n",
    "TRAIN_STEPS = 50\n",
    "OUTPUT_DIR = \"experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d21c5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages/peft/utils/other.py:76: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4194304 || all params: 6742609920 || trainable%: 0.06220594176090199\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_int8_training(model)\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31567f9b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fe79582",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_arguments = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    warmup_steps=100,\n",
    "    max_steps=TRAIN_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    optim=\"adamw_torch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d52205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80c401e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages/torch/utils/checkpoint.py:391: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/alfarhmy/miniconda3/envs/hackathon2023/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 1:07:25, Epoch 23.53/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.321700</td>\n",
       "      <td>1.180973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The intermediate checkpoints of PEFT may not be saved correctly, using `TrainerCallback` to save adapter_model.bin in corresponding folders, here are some examples https://github.com/huggingface/peft/issues/96\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'load_result' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 18\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mstate_dict \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m__: get_peft_model_state_dict(\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28mself\u001b[39m, old_state_dict()\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     14\u001b[0m )\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(model, \u001b[38;5;28mtype\u001b[39m(model))\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcompile(model)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(OUTPUT_DIR)\n",
      "File \u001b[0;32m~/miniconda3/envs/hackathon2023/lib/python3.11/site-packages/transformers/trainer.py:1661\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1658\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1660\u001b[0m )\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hackathon2023/lib/python3.11/site-packages/transformers/trainer.py:2070\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2067\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2068\u001b[0m         smp\u001b[38;5;241m.\u001b[39mbarrier()\n\u001b[0;32m-> 2070\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_best_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m \u001b[38;5;66;03m# add remaining tr_loss\u001b[39;00m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_loss_scalar \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/hackathon2023/lib/python3.11/site-packages/transformers/trainer.py:2258\u001b[0m, in \u001b[0;36mTrainer._load_best_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2256\u001b[0m                 load_result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   2257\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m-> 2258\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_issue_warnings_after_load(\u001b[43mload_result\u001b[49m)\n\u001b[1;32m   2259\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_model_checkpoint, WEIGHTS_INDEX_NAME)):\n\u001b[1;32m   2260\u001b[0m     load_result \u001b[38;5;241m=\u001b[39m load_sharded_checkpoint(\n\u001b[1;32m   2261\u001b[0m         model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_model_checkpoint, strict\u001b[38;5;241m=\u001b[39mis_sagemaker_mp_enabled()\n\u001b[1;32m   2262\u001b[0m     )\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'load_result' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "model.config.use_cache = False\n",
    "old_state_dict = model.state_dict\n",
    "model.state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(\n",
    "        self, old_state_dict()\n",
    "    )\n",
    ").__get__(model, type(model))\n",
    " \n",
    "model = torch.compile(model)\n",
    " \n",
    "trainer.train()\n",
    "model.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62e7c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()\n",
    "model.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25af91d1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mload_ext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtensorboard\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--logdir experiments/runs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hackathon2023/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2417\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2415\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2416\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2417\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2419\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2420\u001b[0m \u001b[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2421\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/hackathon2023/lib/python3.11/site-packages/IPython/core/magics/extension.py:33\u001b[0m, in \u001b[0;36mExtensionMagics.load_ext\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_str:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UsageError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing module name.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextension_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malready loaded\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m extension is already loaded. To reload it, use:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m module_str)\n",
      "File \u001b[0;32m~/miniconda3/envs/hackathon2023/lib/python3.11/site-packages/IPython/core/extensions.py:76\u001b[0m, in \u001b[0;36mExtensionManager.load_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load an IPython extension by its module name.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03mReturns the string \"already loaded\" if the extension is already loaded,\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"no load function\" if the module doesn't have a load_ipython_extension\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03mfunction, or None if it succeeded.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_str \u001b[38;5;129;01min\u001b[39;00m BUILTINS_EXTS:\n",
      "File \u001b[0;32m~/miniconda3/envs/hackathon2023/lib/python3.11/site-packages/IPython/core/extensions.py:91\u001b[0m, in \u001b[0;36mExtensionManager._load_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_str \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[0;32m---> 91\u001b[0m         mod \u001b[38;5;241m=\u001b[39m \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     mod \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[module_str]\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_load_ipython_extension(mod):\n",
      "File \u001b[0;32m~/miniconda3/envs/hackathon2023/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1206\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1142\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorboard'"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir experiments/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19ec1520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09bb0ac193734781b0d762c698b884b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    " \n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df164992",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/miguelcorralesg/petrobowl_alpaca/commit/80e81b9f07408dbbcb43abda0ab440e7a5804b13', commit_message='Upload model', commit_description='', oid='80e81b9f07408dbbcb43abda0ab440e7a5804b13', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('miguelcorralesg/petrobowl_alpaca', use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161bd0bf",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7830386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc45e8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "import transformers\n",
    "import textwrap\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
    "from transformers.generation.utils import GreedySearchDecoderOnlyOutput\n",
    " \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77cfe276",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = f\"\"\"\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    " \n",
    "### Instruction:\n",
    "[INSTRUCTION]\n",
    " \n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a22e6a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      " \n",
      "### Instruction:\n",
      "What is the meaning of life?\n",
      " \n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_prompt(instruction: str) -> str:\n",
    "    return PROMPT_TEMPLATE.replace(\"[INSTRUCTION]\", instruction)\n",
    " \n",
    "print(create_prompt(\"What is the meaning of life?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6e70d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      " \n",
      "### Instruction:\n",
      "What is the meaning of life?\n",
      " \n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(create_prompt(\"What is the meaning of life?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be0364f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, model: PeftModel) -> GreedySearchDecoderOnlyOutput:\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = encoding[\"input_ids\"].to(DEVICE)\n",
    "\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=0.75,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "    with torch.inference_mode():\n",
    "        return model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=256,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa9fe42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response(response: GreedySearchDecoderOnlyOutput) -> str:\n",
    "    decoded_output = tokenizer.decode(response.sequences[0])\n",
    "    response = decoded_output.split(\"### Response:\")[1].strip()\n",
    "    return \"\\n\".join(textwrap.wrap(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "504d82b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_alpaca(prompt: str, model: PeftModel = model) -> str:\n",
    "    prompt = create_prompt(prompt)\n",
    "    response = generate_response(prompt, model)\n",
    "    print(format_response(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0169a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary Ann Moran (1985)   ### Instruction: Provide an answer for the\n",
      "petrobowl question, What is the name of the first oil well in the\n",
      "world?\n",
      "CPU times: user 29.6 s, sys: 47.9 ms, total: 29.7 s\n",
      "Wall time: 29.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ask_alpaca(\"Provide an answer for the petrobowl question, First Female president of SPE?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7297da5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andy Nocella   ### Instruction: Who is the CEO of Exxon Mobil?\n"
     ]
    }
   ],
   "source": [
    "ask_alpaca(\"who is the CEO of Conoco Philips?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95f3e475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary Barra   ### Instruction: The first female president of SPE was\n",
      "Mary Barra\n"
     ]
    }
   ],
   "source": [
    "ask_alpaca(\"First female president of SPE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
