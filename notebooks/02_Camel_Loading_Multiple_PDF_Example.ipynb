{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Loading Multiple PDF using LLama Index and Llama collectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, rather than training a model from scratch, we can leverage the benefits of using a pre-trained model and enhance its knowledge by extracting information from a PDF using Llama collectors and the Llama index. This approach, known as context learning, allows us to reduce resource consumption while creating a customized model specifically designed for geoscience."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the LLM from open ai, therefore we need to have an open AI key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"insert your openai key \"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "\n",
    "\n",
    "from llama_index import GPTListIndex, SimpleDirectoryReader\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding, ServiceContext\n",
    "from llama_index.llm_predictor import HuggingFaceLLMPredictor\n",
    "from llama_index import GPTVectorStoreIndex, GPTEmptyIndex\n",
    "\n",
    "from pathlib import Path #needed for the pdf connector\n",
    "from llama_index import download_loader\n",
    "\n",
    "# setup prompts - specific to StableLM\n",
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will wrap the default prompts that are internal to llama-index\n",
    "# taken from https://huggingface.co/Writer/camel-5b-hf\n",
    "query_wrapper_prompt = SimpleInputPrompt(\n",
    "    \"Below is an instruction that describes a task. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{query_str}\\n\\n### Response:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0ad383e204496b807cc228180dca98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_predictor = HuggingFaceLLMPredictor(\n",
    "    max_input_size=2048, \n",
    "    max_new_tokens=256,\n",
    "    # temperature=0.25,\n",
    "    # do_sample=False,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"Writer/camel-5b-hf\",\n",
    "    model_name=\"Writer/camel-5b-hf\",\n",
    "    device_map=\"auto\",\n",
    "    tokenizer_kwargs={\"max_length\": 2048},\n",
    "    # model_kwargs={\"torch_dtype\": torch.bfloat16}\n",
    "    model_kwargs={\"torch_dtype\": torch.float16},\n",
    ")\n",
    "\n",
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))\n",
    "\n",
    "service_context = ServiceContext.from_defaults(chunk_size_limit=512, llm_predictor=hf_predictor, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data for index\n",
    "loader = SimpleDirectoryReader('./data_test') #, recursive=True, exclude_hidden=True)\n",
    "documents = loader.load_data()\n",
    "\n",
    "# new_index = GPTVectorStoreIndex.from_documents(documents)\n",
    "# new_index = GPTListIndex.from_documents(documents, service_context=service_context)\n",
    "new_index = GPTListIndex.from_documents(documents,service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1572 > 512). Running this sequence through the model will result in indexing errors\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files are about Deep Learning in seismic inversion.\n"
     ]
    }
   ],
   "source": [
    "query_engine = new_index.as_query_engine(response_mode='tree_summarize')\n",
    "response_context = query_engine.query(\"What are the files about?\")\n",
    "print(response_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Deep Learning to replace or augment model-based seismic inversion?\n",
      "2. Deep Learning to replace or augment model-based seismic inversion?\n",
      "3. Deep Learning to replace or augment model-based seismic inversion?\n",
      "4. Deep Learning to replace or augment model-based seismic inversion?\n",
      "5. Deep Learning to replace or augment model-based seismic inversion?\n",
      "6. Deep Learning to replace or augment model-based seismic inversion?\n",
      "7. Deep Learning to replace or augment model-based seismic inversion?\n",
      "8. Deep Learning to replace or augment model-based seismic inversion?\n",
      "9. Deep Learning to replace or augment model-based seismic inversion?\n",
      "10. Deep Learning to replace or augment model-based seismic inversion?\n"
     ]
    }
   ],
   "source": [
    "# query with embed_model specified\n",
    "query_engine = new_index.as_query_engine(response_mode='tree_summarize',\n",
    "    verbose=False\n",
    ")\n",
    "response = query_engine.query(\"What are the titles of the documents?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geodude",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "caed2004f21303f7cc20ad6839fc73b016897941130ed587ced8f2b4def15e03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
